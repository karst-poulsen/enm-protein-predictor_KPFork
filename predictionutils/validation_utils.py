import pandas as pd
import sklearn
from typing import Union

from . import data_utils


class ValidationUtils(object):
    """Several statistical tests used to validate the models predictive power

    Args:
        :param true_results (array of ints): The true testing values
        :param predicted_results (array of floats): The predicted probabilities
        generated by the model
    Attributes:
        :true_results (array of ints): The true testing values
        :predicted_results (array of floats): The predicted probabilities
        generated by the model
    """
    def __init__(self, target: Union[pd.DataFrame, pd.Series], prediction: Union[pd.DataFrame, pd.Series]):
        self.TARGET = target
        self.PREDICTION = prediction

    def well_rounded_validation(self, probability_threshold: float):
        """Calculates the AUROC, Recall, Precision, F1 Score, Accuracy, and
        confusion matrix of the model

        Args:
            None
        Returns:
                (dict) {"AUROC" : (float),
                        "Recall" : (float),
                        "Precision" : (float),
                        "Accuracy" : (float),
                        "Confusion Matrix" : (list)}

        Returns a Dict containing the AUROC, Recall, Precision, F1-score, Accuracy, and confusion matrix from the model
        """
        d = data_utils.DataUtils()
        classified_predictions = d.classify(self.PREDICTION, probability_threshold)
        conf_matrix = sklearn.metrics.confusion_matrix(self.TARGET, classified_predictions, labels=None)

        return {
                "AUROC": float(sklearn.metrics.roc_auc_score(self.TARGET, self.PREDICTION)),
                "Recall": float(sklearn.metrics.recall_score(self.TARGET, classified_predictions, labels=None, pos_label=1, average=None, sample_weight=None)[1]),
                "Precision": float(sklearn.metrics.precision_score(self.TARGET, classified_predictions, labels=None, pos_label=1, average=None, sample_weight=None)[1]),
                "F1 Score": float(sklearn.metrics.f1_score(self.TARGET, classified_predictions)),
                "Accuracy": float(sklearn.metrics.accuracy_score(self.TARGET, classified_predictions)),
                "Confusion Matrix": [int(conf_matrix[0][0]), int(conf_matrix[0][1]), int(conf_matrix[1][0]), int(conf_matrix[1][1])]
                }
